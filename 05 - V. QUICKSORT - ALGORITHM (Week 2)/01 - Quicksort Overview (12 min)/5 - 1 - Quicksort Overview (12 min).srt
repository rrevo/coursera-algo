1
00:00:00,000 --> 00:00:03,764
So now we come to one of my favorite
sequence of lectures, where we going to

2
00:00:03,764 --> 00:00:07,384
discuss the famous QuickSort algorithm.
If you ask professional computer

3
00:00:07,384 --> 00:00:11,622
scientists and professional programmers to
draw up a list of their top five, top ten

4
00:00:11,622 --> 00:00:15,859
favorite algorithms, I'll bet you'd see
QuickSort on many of those, those peoples'

5
00:00:15,859 --> 00:00:19,844
lists. So, why is that? After all, we've
already discussed sorting. We already have

6
00:00:19,844 --> 00:00:23,476
a quite good and practical sorting
algorithm, mainly the Merge Sort

7
00:00:23,476 --> 00:00:27,411
algorithm. Well, QuickSort, in addition to
being very practical, it's competitive

8
00:00:27,411 --> 00:00:31,447
with, and often superior to, Merge Sort.
So, in addition to being very practical,

9
00:00:31,447 --> 00:00:35,684
and used all the time in the real world,
and in programming libraries, it's just a

10
00:00:35,684 --> 00:00:39,878
extremely elegant algorithm. When you see
the code, it's just so succinct. It's so

11
00:00:39,878 --> 00:00:43,782
elegant, you just sorta wish you had come
up with it yourself. Moreover, the

12
00:00:43,782 --> 00:00:47,633
mathematical analysis which explains why
QuickSort runs so fast, and that

13
00:00:47,633 --> 00:00:51,906
mathematical analysis, we'll cover in
detail, is very slick. So it's something

14
00:00:51,906 --> 00:00:55,998
I can cover in just about half an hour or
so. So more precisely what we'll prove

15
00:00:55,998 --> 00:01:00,124
about the QuickSort algorithm is that a
suitable randomized implementation runs in

16
00:01:00,124 --> 00:01:04,104
time N log N on average. And I'll tell
you exactly what I mean by on average,

17
00:01:04,256 --> 00:01:08,262
later on in this sequence of lectures.
And, moreover, the constants hidden in the

18
00:01:08,262 --> 00:01:12,370
Big-Oh notation are extremely small. And,
that'll be evident from the analysis that

19
00:01:12,370 --> 00:01:16,477
we do. Finally, and this is one thing that
differentiates QuickSort from the merge

20
00:01:16,477 --> 00:01:20,483
sort algorithm, is it operates in place.
That is, it needs very little additional

21
00:01:20,483 --> 00:01:24,641
storage, beyond what's given in the input
array, in order to accomplish the goal of

22
00:01:24,641 --> 00:01:28,850
sorting. Essentially, what QuickSort does
is just repeated swaps within the space of

23
00:01:28,850 --> 00:01:32,501
the input array, until it finally
concludes with a sorted version of the

24
00:01:32,501 --> 00:01:36,092
given array. The final thing I want to
mention on this first slide is that,

25
00:01:36,092 --> 00:01:39,819
unlike most of the videos, this set of the
videos will actually have an

26
00:01:39,819 --> 00:01:43,845
accompanying set of lecture notes, which
I've posted on, in PDF, from the course

27
00:01:43,845 --> 00:01:47,821
website. Those are largely, redundant.
They're optional, but if you want another

28
00:01:47,821 --> 00:01:51,797
treatment of what I'm gonna discuss, a
written treatment, I encourage you to look

29
00:01:51,797 --> 00:01:55,574
at the lecture notes, on the course
website. So, for the rest of this video,

30
00:01:55,574 --> 00:01:59,550
I'm gonna give you an overview of the
ingredients of QuickSort, and what we have

31
00:01:59,550 --> 00:02:03,526
to discuss in more detail, and the rest of
the lectures will give details of the

32
00:02:03,526 --> 00:02:08,875
implementation, as well as the
mathematical analysis. So let's begin by

33
00:02:08,875 --> 00:02:13,445
recalling the sorting problem. This is
exactly the same problem we discussed back

34
00:02:13,445 --> 00:02:17,677
when we covered Merge Sort. So we're given
as input an array of n numbers in

35
00:02:17,677 --> 00:02:32,635
arbitrary order. So, for example, perhaps
the input looks like this array here. And

36
00:02:32,635 --> 00:02:37,462
then what do we gotta do? We just gotta
output a version of these same numbers but

37
00:02:37,462 --> 00:02:44,596
in increasing order. Like when we
discussed Merge Sort, I'm gonna make a

38
00:02:44,596 --> 00:02:48,526
simplifying assumption just to keep the
lectures as simple as possible. Namely I'm

39
00:02:48,526 --> 00:02:52,216
going to assume the input array has no
duplicates. That is, all of the entries

40
00:02:52,216 --> 00:02:58,943
are distinct. And like with the merge
sort, I encourage you to think about how

41
00:02:58,943 --> 00:03:04,085
you would alter the implementation of
QuickSort so that it deals correctly with

42
00:03:04,085 --> 00:03:12,023
ties, with duplicate entries. To discuss
how QuickSort works at a high-level, I need

43
00:03:12,023 --> 00:03:16,888
to introduce you to the key subroutine, and
this is really the, key great idea in

44
00:03:16,888 --> 00:03:21,753
QuickSort, which is to use a subroutine
which partitions the array around a pivot

45
00:03:21,753 --> 00:03:30,018
element. So what does this mean? Well,
the first thing you gotta do is, you gotta

46
00:03:30,018 --> 00:03:37,779
pick one element in your array to act as a
pivot element. Now eventually we'll worry

47
00:03:37,779 --> 00:03:42,342
quite a bit about exactly how we choose
this magical pivot element. But for now

48
00:03:42,342 --> 00:03:47,309
you can just think of it that we pluck out
the very first element in the array to act

49
00:03:47,309 --> 00:03:54,349
as the pivot. So, for example, in the
input array that I mentioned on the

50
00:03:54,349 --> 00:04:01,750
previous slide, we could just use "3" as
the pivot element. After you've chosen a

51
00:04:01,750 --> 00:04:06,369
pivot element, you then re-arrange the
array, and re-arrange it so that every, all

52
00:04:06,369 --> 00:04:11,278
the elements which come to the left of the
pivot element are less than the pivot, and

53
00:04:11,278 --> 00:04:15,840
all the elements which come after the
pivot element are greater than the pivot.

54
00:04:16,740 --> 00:04:22,387
So for example, given this input array,
one legitimate way to rearrange it, so that

55
00:04:22,387 --> 00:04:30,220
this holds, is the following. Perhaps in
the first two entries, we have the 2 and

56
00:04:30,220 --> 00:04:35,890
the 1. Then comes the pivot element. And
then comes the elements 4 through 8

57
00:04:35,890 --> 00:04:42,882
in some perhaps jingled order. So
notice that the elements to the left of

58
00:04:42,882 --> 00:04:50,684
the pivot, the 2 and the 1, are indeed
less than the pivot, which is 3.

59
00:04:50,684 --> 00:04:54,436
And the five elements to the right of the
pivot, to the right of the 3, are

60
00:04:54,436 --> 00:05:00,689
indeed all greater than 3. Notice in
the Partition subroutine, we do not

61
00:05:00,689 --> 00:05:04,689
insist that we get the relative order
correct amongst those elements less than

62
00:05:04,689 --> 00:05:08,485
the pivot, or amongst those elements
bigger than the pivot. So, in some sense,

63
00:05:08,485 --> 00:05:12,636
we're doing some kind of partial sorting.
We're just bucketing the elements of the

64
00:05:12,636 --> 00:05:16,635
array into one bucket, those less than
the pivot, and then a second bucket, those

65
00:05:16,635 --> 00:05:20,736
bigger than the pivot. And we don't care
about, getting right the order amongst

66
00:05:20,736 --> 00:05:27,685
each, within each of those two buckets.
So, partitioning is certainly a more

67
00:05:27,685 --> 00:05:31,877
modest goal than sorting, but it does make
progress toward sorting. In particular,

68
00:05:31,877 --> 00:05:35,910
the pivot element itself winds up in its
rightful position. That is, the pivot

69
00:05:35,910 --> 00:05:40,311
element winds up where it should be in the
final sorted version of the array. You'll

70
00:05:40,311 --> 00:05:44,345
notice in the example, we chose as the
pivot the third largest element, and it

71
00:05:44,345 --> 00:05:48,169
does, indeed, wind up in the third
position of the array. So, more generally,

72
00:05:48,169 --> 00:05:52,308
where should the pivot be in the final
sorted version? Well, it should be to the

73
00:05:52,308 --> 00:05:56,447
right of everything less than it. It
should be to the left of everything bigger

74
00:05:56,447 --> 00:06:03,537
than it. And that's exactly what
partitioning does, by definition. So, why

75
00:06:03,537 --> 00:06:06,910
is it such a good idea to have a
partitioning subroutine? After all, we

76
00:06:06,910 --> 00:06:10,765
don't really care about partitioning. What
we want to do is sort. Well, the point is

77
00:06:10,765 --> 00:06:14,764
that partitioning can be done quickly. It
can be done in linear time. And it's a way

78
00:06:14,764 --> 00:06:18,812
of making progress toward having a sorted
version of an array. And it's gonna enable

79
00:06:18,812 --> 00:06:22,474
a divide-and-conquer approach toward
sorting the input array. So, in a little

80
00:06:22,474 --> 00:06:26,088
bit more detail, let me tell you about two
cool facts about the Partition

81
00:06:26,088 --> 00:06:29,943
subroutine. I'm not gonna give you the
code for partitioning here. I'm gonna give

82
00:06:29,943 --> 00:06:33,557
it to you on the next video. But, here are
the two salient properties of the

83
00:06:33,557 --> 00:06:41,447
Partition subroutine, discussed in detail
in the next video. So the first cool

84
00:06:41,447 --> 00:06:49,497
fact is that it can be implemented in
linear, that, is O(N) time, where N

85
00:06:49,497 --> 00:06:53,833
is the size of the input array, and moreover,
not just linear time but linear time

86
00:06:53,833 --> 00:06:58,012
with essentially no extra overhead. So
we're gonna get a linear time of mutation,

87
00:06:58,012 --> 00:07:02,243
where all you do is repeated swaps. You do
not allocate any additional memory. And

88
00:07:02,243 --> 00:07:09,491
that's key to the practical performance of
the QuickSort algorithm. [sound] Secondly,

89
00:07:09,491 --> 00:07:14,820
it cuts down the problem size, so it
enables the divide-and-conquer approach.

90
00:07:17,540 --> 00:07:21,916
Namely, after we've partitioned an array
around some pivot elements, all we have to

91
00:07:21,916 --> 00:07:25,727
do is recursively sort the elements that
lie on the left of the pivot. And

92
00:07:25,727 --> 00:07:30,000
recursively sort the elements that lie on
the right of the pivot. And then, we'll be

93
00:07:30,000 --> 00:07:34,120
done. So, that leads us to the high-level
description of the QuickSort algorithm.

94
00:07:35,720 --> 00:07:40,643
Before I give the high-level description,
I should mention that this, algorithm was

95
00:07:40,643 --> 00:07:45,335
discovered by, Tony Hoare, roughly, 1961
or so. This was at the very beginning of

96
00:07:45,335 --> 00:07:50,200
Hoare's career. He was just about 26, 27
years old. He went on to do a lot of other

97
00:07:50,200 --> 00:07:54,776
contributions, and, eventually wound up
winning the highest honor in computer

98
00:07:54,776 --> 00:07:59,584
science, the ACM Turing Award, in 1980.
And when you see this code, I'll bet you

99
00:07:59,584 --> 00:08:04,334
feel like you wish you had come up with
this yourself. It's hard not to be envious

100
00:08:04,334 --> 00:08:11,291
of the inventor of this very elegant QuickSort
algorithm. So, just like in Merge Sort,

101
00:08:11,293 --> 00:08:15,716
this is gonna be a divide-and-conquer
algorithm. So it takes an array of some

102
00:08:15,716 --> 00:08:20,311
length N, and if it's an array of length
N, it's already sorted, and that's the

103
00:08:20,311 --> 00:08:27,850
base case and we can return. Otherwise
we're gonna have two recursive calls. The

104
00:08:27,850 --> 00:08:32,052
big difference from Merge Sort is that,
whereas in Merge Sort, we first split the

105
00:08:32,052 --> 00:08:35,939
array into pieces, recourse, and then
combine the results, here, the recursive

106
00:08:35,939 --> 00:08:40,088
calls come last. So, the first thing we're
going to do is choose a pivot element,

107
00:08:40,088 --> 00:08:44,448
then partition the array around that pivot
element, and then do two recursive calls.

108
00:08:44,448 --> 00:08:51,175
And then, we'll be done. There will be no
combined step, no merge step. So in the

109
00:08:51,175 --> 00:08:54,861
general case, the first thing you do is
choose a pivot element. For the moment I'm

110
00:08:54,861 --> 00:08:58,412
going to be loose, leave the ChoosePivot
subroutine unimplemented. There's going to

111
00:08:58,412 --> 00:09:02,189
be an interesting discussion about exactly
how you should do this. For now, you just

112
00:09:02,189 --> 00:09:05,558
do it in some way, that for somehow you
come up with one pivot element. For

113
00:09:05,558 --> 00:09:12,504
example, a naive way would be to just
choose the first element. Then you invoke

114
00:09:12,504 --> 00:09:21,812
the Partition subroutine that we'll discuss
in the last couple slides. [sound]. So

115
00:09:21,812 --> 00:09:26,522
recall that the results in a version of
the array in which the pivot element p is

116
00:09:26,522 --> 00:09:31,061
in its rightful position, everything to
the left of p is less than p, everything

117
00:09:31,061 --> 00:09:35,771
to the right of the pivot is bigger than
the pivot, and then all you have to do to

118
00:09:35,771 --> 00:09:43,605
finish up is recurse on both sides. So
let's call the elements less than p the

119
00:09:43,605 --> 00:09:48,979
first part of the partitioned array, and
the elements greater than p the second

120
00:09:48,979 --> 00:09:57,311
part of the recursive array.  And now
we just call QuickSort again to

121
00:09:57,311 --> 00:10:04,417
recursively sort the first part, and then
the, recursively sort the second part. And

122
00:10:04,417 --> 00:10:08,423
that is it. That is the entire QuickSort
algorithm at the high-level. This is one

123
00:10:08,423 --> 00:10:12,577
of the relatively rare recursive, divide-
and-conquer algorithms that you're going

124
00:10:12,577 --> 00:10:16,434
to see, where you literally do no work
after solving the sub-problems. There is

125
00:10:16,434 --> 00:10:20,292
no combine step, no merge step. Once
you've partitioned, you just sort the two

126
00:10:20,292 --> 00:10:26,001
sides and you're done. So that's the high-
level description of the QuickSort

127
00:10:26,012 --> 00:10:29,803
algorithm. Let me give you a quick tour of
what the rest of the video's going to be

128
00:10:29,803 --> 00:10:33,594
about. So first of all I owe you details
on this Partition subroutine. I promise

129
00:10:33,594 --> 00:10:37,340
you it can be implemented in linear time
with no additional memory. So I'll show

130
00:10:37,340 --> 00:10:41,040
you an implementation of that on the next
video. We'll have a short video that

131
00:10:41,040 --> 00:10:44,723
formally proves correctness of the
QuickSort algorithm. I think most of you

132
00:10:44,723 --> 00:10:48,265
will kinda see intuitively why it's
correct. So, that's a video you can skip

133
00:10:48,265 --> 00:10:51,995
if you'd want. But if you do want to see
what a formal proof of correctness for a

134
00:10:51,995 --> 00:10:55,631
divide-and-conquer algorithm looks like,
you might want to check out that video.

135
00:10:55,631 --> 00:10:59,503
Then, we'll be discussing exactly how the
pivot is chosen. It turns out the running

136
00:10:59,503 --> 00:11:03,281
time of QuickSort depends on what pivot
you choose. So, we're gonna have to think

137
00:11:03,281 --> 00:11:06,681
carefully about that. Then, we'll
introduce randomized QuickSort, which is

138
00:11:06,681 --> 00:11:10,506
where you choose a pivot element uniformly
at random from the given array, hoping

139
00:11:10,506 --> 00:11:14,615
that a random pivot is going to be pretty
good, sufficiently often. And then we'll

140
00:11:14,615 --> 00:11:18,611
give the mathematical analysis in three parts.
We'll prove that the QuickSort algorithm runs in

141
00:11:18,611 --> 00:11:22,475
N log N time, with small constants, on
average, for a randomly chosen pivot. In

142
00:11:22,475 --> 00:11:26,634
the first analysis video, I'll introduce a
general decomposition principle of how you

143
00:11:26,634 --> 00:11:30,547
take a complicated random variable, break
it into indicator random variables, and

144
00:11:30,547 --> 00:11:34,070
use linearity of expectation to get a
relatively simple analysis. That's

145
00:11:34,070 --> 00:11:37,983
something we'll use a couple more times in
the course. For example, when we study

146
00:11:37,983 --> 00:11:42,093
hashing. Then, we'll discuss sort of the
key insight behind the QuickSort analysis,

147
00:11:42,093 --> 00:11:45,762
which is about understanding the
probability that a given pair of elements

148
00:11:45,762 --> 00:11:49,459
gets compared at some point in the
algorithm. That'll be the second part. And

149
00:11:49,459 --> 00:11:53,003
then there's going to be some mathematical
computations just to sort of tie

150
00:11:53,003 --> 00:11:56,874
everything together and that will give us
the bound the QuickSort running time.

151
00:11:56,874 --> 00:12:00,698
Another video that's available is a review
of some basic probability concepts for

152
00:12:00,698 --> 00:12:04,708
those of you that are rusty, and they will be
using in the analysis of QuickSort. Okay?

153
00:12:04,708 --> 00:12:07,460
So that's it for the overview, let's move
on to the details.
