1
00:00:00,000 --> 00:00:04,320
Having completed our first analysis of an
algorithm, namely an upper bound on the

2
00:00:04,320 --> 00:00:08,746
running time of the Merge Short algorithm.
What I wanna do next is take a step back,

3
00:00:08,746 --> 00:00:13,173
and be explicit about three assumptions,
three biases that we made when we did this

4
00:00:13,173 --> 00:00:17,386
analysis of Merge Short, and interpreted
the results. These three assumptions we

5
00:00:17,386 --> 00:00:21,600
will adopt as guiding principles for how
to reason about algorithms, and how to

6
00:00:21,600 --> 00:00:25,973
define a so called fast algorithm for the
rest of the course. So, the first guiding

7
00:00:25,973 --> 00:00:30,240
principle is that we used what's often
called worst case analysis. By worst case.

8
00:00:30,240 --> 00:00:34,715
Analysis, I simply mean that our upper
bound of six N log N plus six N. Applies

9
00:00:34,715 --> 00:00:39,533
to the number of lines of executed for
every single input array of length end. We

10
00:00:39,533 --> 00:00:44,352
made absolutely no assumptions about the
input, where it comes from, what it looks

11
00:00:44,352 --> 00:00:48,932
like beyond what the input length N was.
Put differently, if, hypothetically, we

12
00:00:48,932 --> 00:00:53,869
had some adversary whose sole purpose in
life was to concoct some malevolent input

13
00:00:53,869 --> 00:00:58,627
designed to make our algorithm run as slow
as possible. The worst this adversary

14
00:00:58,627 --> 00:01:03,367
could do, is. Upper bounded by this same
number, 6N log N + 6N. Now, this, so,

15
00:01:03,367 --> 00:01:08,177
sort of worst case guarantee popped out so
naturally from our analysis of Merge

16
00:01:08,177 --> 00:01:12,987
Short, you might well be wondering, what
else could you do? Well, two other methods

17
00:01:12,987 --> 00:01:17,677
of analysis, which do have their place,
although we won't really dicuss them in

18
00:01:17,677 --> 00:01:22,366
this course, are quote unquote, average
case analysis. And also the use of a set

19
00:01:22,366 --> 00:01:26,637
of prespecified benchmarks. By average
case analysis, I mean, you analyze the

20
00:01:26,637 --> 00:01:30,938
average running time of an algorithm under
some assumption about the relative

21
00:01:30,938 --> 00:01:35,350
frequencies of different inputs. So, for
example, in the sorting problem, one thing

22
00:01:35,350 --> 00:01:39,983
you could do, although it's not what we do
here. You could assume that every possible

23
00:01:39,983 --> 00:01:44,395
input array is equally unlikely, and
then analyze the average running time of

24
00:01:44,395 --> 00:01:48,751
an algorithm. By benchmarks, I just mean
that one agrees up front about some set,

25
00:01:48,751 --> 00:01:53,150
say ten or twenty, Benchmark inputs, which
are thought to represent practical or

26
00:01:53,150 --> 00:01:57,347
typical inputs for the algorithm. Now,
both average-case analysis and benchmarks

27
00:01:57,347 --> 00:02:01,438
are useful in certain settings, but for
them to make sense, you really have to

28
00:02:01,438 --> 00:02:05,688
have domain knowledge about your problem.
You need to have some understanding of

29
00:02:05,688 --> 00:02:09,778
what inputs are more common than others,
what inputs better represent typical

30
00:02:09,778 --> 00:02:13,763
inputs than others. By contrast, in
worst-case analysis, by definition you're

31
00:02:13,763 --> 00:02:17,694
making absolutely no assumptions about
where the input comes from. So, as a

32
00:02:17,694 --> 00:02:20,828
result, worst-case analysis is
particularly appropriate for

33
00:02:20,828 --> 00:02:25,456
general-purpose sub-routines. Sub-routines
that you design. Find without having any

34
00:02:25,456 --> 00:02:30,192
knowledge of how they will be used or what
kind of inputs they will be used on. And

35
00:02:30,192 --> 00:02:34,556
happily, another bonus of doing worst case
analysis, as we will in this course, it's

36
00:02:34,556 --> 00:02:38,388
usually mathematically much more
attractable than trying to analyze the

37
00:02:38,388 --> 00:02:42,539
average performance of an algorithm under
some distribution over inputs. Or to

38
00:02:42,539 --> 00:02:46,903
understand the detailed behavior of an
algorithm on a particular set of benchmark

39
00:02:46,903 --> 00:02:51,161
inputs. This mathemetical tractabilty
was reflected in our Merge Sort analysis,

40
00:02:51,161 --> 00:02:55,258
where we had no a priori goal of
analyzing the worst case, per se. But it's

41
00:02:55,258 --> 00:02:59,630
naturally what popped out of our reasoning
about the algorithm's running time. The

42
00:02:59,630 --> 00:03:03,976
second and third guiding principles are
closely related. The second one is that,

43
00:03:03,976 --> 00:03:07,827
in this course, when we analyze
algorithms, we won't worry unduly about

44
00:03:07,827 --> 00:03:12,228
small constant factors or lower order
terms. We saw this philosophy at work very

45
00:03:12,228 --> 00:03:16,629
early on in our analysis of merge sort.
When we discussed the number of lines of

46
00:03:16,629 --> 00:03:21,015
code that the merge subroutine requires.
We first upper-bounded it by 4m plus two,

47
00:03:21,015 --> 00:03:25,120
for an array of length m, and then we
said, eh, let's just think about it as 6m

48
00:03:25,120 --> 00:03:29,118
instead. Let's have a simpler, sloppy
upper-bound and work with that. So, that

49
00:03:29,118 --> 00:03:33,116
was already an example of not worrying
about small changes in the constant

50
00:03:33,116 --> 00:03:37,327
factor. Now, the question you should be
wondering about is, why do we do this, and

51
00:03:37,327 --> 00:03:41,204
can we really get away with it? So let me
tell you about the justifications for this

52
00:03:41,204 --> 00:03:45,430
guiding principle. So the first motivation
is clear, and we used it already in our

53
00:03:45,430 --> 00:03:49,837
merge short analysis. Which is simply way
easier mathematically, if we don't have

54
00:03:49,837 --> 00:03:54,190
to, precisely pin down what the [inaudible] constant factors and lower-order terms are.

55
00:03:54,190 --> 00:03:58,479
The second justification is a little less
obvious, but is extremely important. So, I

56
00:03:58,479 --> 00:04:02,715
claim that, given the level at which we're
describing and analyzing algorithms in

57
00:04:02,715 --> 00:04:06,325
this course, it would be totally
inappropriate to obsess unduly about

58
00:04:06,325 --> 00:04:10,143
exactly what the constant factors are.
Recall our discussion of the merge

59
00:04:10,143 --> 00:04:13,961
subroutine. So, we wrote that subroutine
down in pseudocode, and we gave it

60
00:04:13,961 --> 00:04:18,250
analysis of 4m plus two on the number of
lines of code executed, given an input of

61
00:04:18,250 --> 00:04:22,278
length m. We also noted that, it was
somewhat ambiguous exactly how many lines

62
00:04:22,278 --> 00:04:26,677
of code we should count it as, depending
on how you count loop increments and so on. So

63
00:04:26,677 --> 00:04:30,811
even there it's small constant factors
could creep in given the under

64
00:04:30,811 --> 00:04:35,064
specification of the pseudo code.
Depending on how that pseudo code gets

65
00:04:35,064 --> 00:04:39,921
translated into an actual program language
like C or Java. You'll see the number of

66
00:04:39,921 --> 00:04:44,330
lines of code deviate even further, not
but a lot but again by small constant

67
00:04:44,330 --> 00:04:48,625
factors. When such a program is then
compiled down into machine code, you'll

68
00:04:48,625 --> 00:04:52,977
see even greater variance depending on the
exact processor, the compiler, the

69
00:04:52,977 --> 00:04:56,985
compiler optimizations, the programming
implementation, and so on. So to

70
00:04:56,985 --> 00:05:01,843
summarize, because we're going to describe
algorithms at a level. That transcends any

71
00:05:01,843 --> 00:05:06,438
particular programming language. It would
be inappropriate to specify precise

72
00:05:06,438 --> 00:05:11,092
constants. The precise constants were
ultimately determined by more machine

73
00:05:11,092 --> 00:05:15,567
dependent aspects like who the programmer
is, what the compiler is, what the

74
00:05:15,567 --> 00:05:20,289
processor is, and so on. And now the third
justification is frankly, we're just going

75
00:05:20,289 --> 00:05:25,415
to be able to get away with it. [sound]
That is, one might be concerned that

76
00:05:25,415 --> 00:05:29,994
ignoring things like small constant factors
leads us astray. That we wind up deriving

77
00:05:29,994 --> 00:05:34,683
results which suggest that an algorithm is
fast when it's really slow in practice, or

78
00:05:34,683 --> 00:05:38,985
vice versa. And for the problems we
discuss in this course we'll get extremely

79
00:05:38,985 --> 00:05:43,212
accurate predictive power. Even though we
won't be keeping track of lower terms and

80
00:05:43,212 --> 00:05:47,180
constant factors. When the mathematical
analysis we do suggests that an algorithm

81
00:05:47,180 --> 00:05:50,806
is fast, indeed it will be. When it
suggests that it's not fast, indeed that

82
00:05:50,806 --> 00:05:54,627
will be the case. So we lose a little bit
of granularity of information. But we

83
00:05:54,627 --> 00:05:58,448
don't lose in what we really care about,
which is accurate guidance about what

84
00:05:58,448 --> 00:06:02,221
algorithms are gonna be faster than
others. So the first two justifications, I

85
00:06:02,221 --> 00:06:06,287
think, are pretty self evident. This third
justification is more of an assertion, but

86
00:06:06,287 --> 00:06:10,304
it's one we'll be baking up over and over
again as we proceed through this course.

87
00:06:10,304 --> 00:06:13,979
Now, don't get me wrong. I'm not saying
constant factors aren't important in

88
00:06:13,979 --> 00:06:18,330
practice. Obviously, for crucial programs
the constant factors are hugely important.

89
00:06:18,330 --> 00:06:22,776
If you're running the sorta crucial loop,
you know, your start up's survival depends

90
00:06:22,776 --> 00:06:26,633
on, by all means optimize the constant
like crazy. The point is just that

91
00:06:26,633 --> 00:06:30,973
understanding tiny constant factors in the
analysis is an inappropriate level of

92
00:06:30,973 --> 00:06:35,205
granularity for the kind of algorithm
analysis we're going to be doing in this

93
00:06:35,205 --> 00:06:39,696
course. Okay, lets move on the, the third
and final guiding principle. So the third

94
00:06:39,696 --> 00:06:44,061
principle is that we're going to use
what's called asymptotic analysis, by

95
00:06:44,061 --> 00:06:49,016
which I mean we will focus on the case of
a large input sizes. The performance of an

96
00:06:49,016 --> 00:06:53,671
algorithm as the size N of the input grows
large, that is, tends to infinity. Now

97
00:06:53,671 --> 00:06:58,299
this focus on large input size is it was
already evident when we interpreted our

98
00:06:58,299 --> 00:07:02,523
bound on Merge Sort. So, how did we
describe the bound on Merge Sort? We said,

99
00:07:02,523 --> 00:07:07,209
oh, well, it needs a number of operations
proportional, a constant fact or times in

100
00:07:07,209 --> 00:07:11,721
login. And we very cavalierly declared
that this was better than any algorithm

101
00:07:11,721 --> 00:07:16,350
which has quadratic dependence of it's
running time on the number of operations.

102
00:07:16,350 --> 00:07:21,017
So for example, we argued that merge sort
is a better, faster algorithm than

103
00:07:21,017 --> 00:07:25,934
something like insertion sort, without
actually discussing the constant factors

104
00:07:25,934 --> 00:07:31,185
at all. So mathematically. We were saying
the running time of merge short, which we

105
00:07:31,185 --> 00:07:36,676
know, which we can represent as the
function. Six N log base two of N + 6N

106
00:07:36,676 --> 00:07:41,315
is better than any function which has a
quadratic dependence on n.  Even one with a

107
00:07:41,315 --> 00:07:45,955
small constant like lets say 1/2 N squared which might roughly be the running

108
00:07:45,955 --> 00:07:50,640
time of insertion sort. And this is a
mathematical statement that is true if and

109
00:07:50,640 --> 00:07:55,230
only if N is sufficiently large once N
grows large it is certainly true that

110
00:07:55,230 --> 00:08:00,050
the expression on the left is smaller than
the expression on the right but for small

111
00:08:00,050 --> 00:08:04,926
N the expression on the right is actually
going to be smaller because of the smaller

112
00:08:04,926 --> 00:08:09,689
leading term so in saying that merge sort
is superior to insertion sort the bias

113
00:08:09,689 --> 00:08:14,566
is that we're focusing on problems with a
large N so the question you should have is

114
00:08:14,566 --> 00:08:18,983
that reasonable is that a justified
assumption to focus on large input sizes

115
00:08:18,983 --> 00:08:23,320
and the answer is certainly yes. So the
reason we focus on large input sizes is

116
00:08:23,320 --> 00:08:27,364
because, frankly, those are the only
problems which are even, which are at all

117
00:08:27,364 --> 00:08:31,780
interesting. If all you need to do is sort
100 numbers, use whatever method you want,

118
00:08:31,780 --> 00:08:36,089
and it's gonna happen instantaneously on
modern computers. You don't need to know

119
00:08:36,089 --> 00:08:40,392
say, the divide and conquer paradigm, if
all you need to do is sort 100 numbers. So

120
00:08:40,392 --> 00:08:44,563
one thing you might be wondering is if,
with computers getting faster all the time

121
00:08:44,563 --> 00:08:48,276
according to Moore's Law, if really, it
doesn't even matter to think about

122
00:08:48,276 --> 00:08:52,142
algorithmic analysis, if eventually all
problem sizes will just be trivially

123
00:08:52,142 --> 00:08:56,129
solvable on super fast computers. But, in
fact, the opposite is true. Moore's Law,

124
00:08:56,129 --> 00:09:00,432
with computers getting faster, actually
says that our computational ambitions will

125
00:09:00,432 --> 00:09:04,841
naturally grow. We naturally focus on ever
larger problem sizes. And the gulf between

126
00:09:04,841 --> 00:09:08,672
an N squared algorithm and an m log n
algorithm will become ever wider. A

127
00:09:08,672 --> 00:09:12,765
different way to think about it is in
terms of how much bigger a problem size

128
00:09:12,765 --> 00:09:17,047
you can solve. As computers get faster. If
you are using an algorithm with a running

129
00:09:17,047 --> 00:09:21,160
time which is proportional to the input
size then the computers get faster by a

130
00:09:21,160 --> 00:09:25,531
factor of four then you can solve problems
that are factor of four or larger. Whereas

131
00:09:25,531 --> 00:09:29,592
if you are using an algorithm whose
running time is proportional to the square

132
00:09:29,592 --> 00:09:33,654
of the input size then a computer gets
faster by a factor of four, you can only

133
00:09:33,654 --> 00:09:37,716
solve double the problem size and we'll
see even starker examples of this gulf

134
00:09:37,716 --> 00:09:42,177
between different algorithm approaches as
the time goes on. So to drive this point

135
00:09:42,177 --> 00:09:47,080
home. Let me show you a couple of graphs.
So what we're looking at here, is we're

136
00:09:47,080 --> 00:09:53,036
looking at a graph, of two functions. So
the solid function. Is the upper bound

137
00:09:53,036 --> 00:10:01,841
that we proved on merge sort. So this is
gonna be 6nlog(base2)n plus 6n. And the

138
00:10:01,841 --> 00:10:08,001
dotted line is an estimate. A rather
generous estimate about the running time

139
00:10:08,001 --> 00:10:12,218
of, [inaudible] sort. Namely one-half
times N. Squared. And we see here in the

140
00:10:12,218 --> 00:10:16,547
graph exactly the behavior that we
discussed earlier, which is that the small

141
00:10:16,547 --> 00:10:23,124
N. Down here. In fact because one-half N.
Squared has a smaller leading constant

142
00:10:23,124 --> 00:10:29,459
it's actually a smaller function. And this
is true up to this crossing point of maybe

143
00:10:29,459 --> 00:10:36,000
90 or so. Again, beyond n=90. The
quadratic growth in the N squared term.

144
00:10:36,000 --> 00:10:40,314
Overwhelms the fact that it had a smaller
constant and it starts being bigger than

145
00:10:40,314 --> 00:10:44,628
this other function six of N + six N so in
the regime below 90 it's predicting that

146
00:10:44,628 --> 00:10:48,942
the insertion store will be better and in
the regime above 90 it's predicting that

147
00:10:48,942 --> 00:10:53,048
merge sort will be faster. Now here's
what's interesting let's scale the X axis

148
00:10:53,048 --> 00:10:57,414
let's look well beyond this crossing point
of 90 let's just increase it in order of

149
00:10:57,414 --> 00:11:01,676
magnitude up to a raise in size 1500. And
I want to emphasize these are still very

150
00:11:01,676 --> 00:11:05,574
small problem sizes. If all you need to do is
sort arrays of size 1500 you really don't

151
00:11:05,574 --> 00:11:09,053
need to know Divide-and-conquer
or anything else we'll talk about -- that's a

152
00:11:09,053 --> 00:11:12,901
pretty trivial problem on modern
computers. [sound]. So what we're seeing

153
00:11:12,901 --> 00:11:16,904
is, that even for very modest problem
sizes here, array of, of, size, say 1500.

154
00:11:17,060 --> 00:11:21,271
The quadratic dependence in the insertion
sort bound is more than dwarfing the

155
00:11:21,271 --> 00:11:25,690
fact, that it had a lower constant factor.
So in this large regime, the gulf between

156
00:11:25,690 --> 00:11:29,693
the two algorithms is growing. And of
course, if I increased it another 10X or

157
00:11:29,693 --> 00:11:33,644
100x or 1000x to get to genuinely
interesting problem sizes, the gap between

158
00:11:33,644 --> 00:11:37,647
these two algorithms would be even bigger,
it would be huge. That said, I'm not

159
00:11:37,647 --> 00:11:41,806
saying you should be completely ignorant
of constant factors when you implement

160
00:11:41,806 --> 00:11:45,810
algorithms. It's still good to have a
general sense of what these constant

161
00:11:45,810 --> 00:11:50,060
factors are so for example in highly tuned
versions of Merge Sort which you'll find

162
00:11:50,060 --> 00:11:53,802
in mny programming libraries. In
fact, because of the difference in

163
00:11:53,802 --> 00:11:57,967
constant factors, the algorithm will
actually switch from Merge Sort over to

164
00:11:57,967 --> 00:12:02,240
insertion sort, once the problem size
drops below some particular threshold, say

165
00:12:02,240 --> 00:12:06,405
seven elements, or something like that. So
for small problem sizes, you use the

166
00:12:06,405 --> 00:12:10,841
algorithm with smaller constant factors,
and the insertion sort for larger problem

167
00:12:10,841 --> 00:12:15,095
sizes, you use the algorithm and better
rate of growth, mainly merge short. So, to

168
00:12:15,095 --> 00:12:19,418
review our first guiding principal is that
we're going to pursue worse case analysis.

169
00:12:19,418 --> 00:12:23,232
We're going to look to bounds on the
performance, on the running time of an

170
00:12:23,232 --> 00:12:26,742
algorithm which make no domain
assumptions, which make no assumptions

171
00:12:26,742 --> 00:12:30,861
about which input of a given length the
algorithm is provided. The second guiding

172
00:12:30,861 --> 00:12:34,930
principal is we're not going to focus on
constant factors or lower returns, that

173
00:12:34,930 --> 00:12:38,999
would be inappropriate, given the level of
granularity at which we're describing

174
00:12:38,999 --> 00:12:43,119
algorithms and third is where going to
focus on the rate of growth of algorithms

175
00:12:43,119 --> 00:12:47,262
for large problem sizes. Putting these
three principles together, we get a

176
00:12:47,262 --> 00:12:51,437
mathematical definition of a fast
algorithm. Namely, we're gonna pursue

177
00:12:51,437 --> 00:12:56,267
algorithms whose worst case running time
grows slowly as a function of the input

178
00:12:56,267 --> 00:13:01,039
size. So let me tell you how you should
interpret what I just wrote down in this

179
00:13:01,039 --> 00:13:05,452
box. So on the left hand side is clearly
what we want. Okay, we want algorithms

180
00:13:05,452 --> 00:13:09,626
which run quickly if we implement them.
And on the right hand side is a proposed

181
00:13:09,626 --> 00:13:13,331
mathematical surrogate of a fast
algorithm. Right, the left hand side is

182
00:13:13,331 --> 00:13:17,557
not a mathematical definition. The right
hand side is, as well become clear in the

183
00:13:17,557 --> 00:13:21,731
next set of lectures. So we're identifying
fast algorithms, which, those that have

184
00:13:21,731 --> 00:13:25,801
good asymptotic running time which grows
slowly with the input size. Now, what

185
00:13:25,801 --> 00:13:29,750
would we want from a mathematical
definition? We'd want a sweet spot. On one

186
00:13:29,750 --> 00:13:34,327
hand we want something we can actually
reason about. This is why we zoom out and

187
00:13:34,327 --> 00:13:39,075
squint and ignore things like constant
factors and lower terms. We can't keep

188
00:13:39,075 --> 00:13:43,514
track of everything. Otherwise we'd never
be able to analyze stuff. On the other hand

189
00:13:43,514 --> 00:13:47,323
we don't want to throw out the baby with
the bath water, we want to retain

190
00:13:47,323 --> 00:13:51,441
predictive power and this turns out this
definition turns out for the problems

191
00:13:51,441 --> 00:13:55,662
we're going to talk about in this course,
to be the sweet spot for reasoning about

192
00:13:55,662 --> 00:13:59,780
algorithms okay worst case analysis using
the asymptotic running time. We'll be able to

193
00:13:59,780 --> 00:14:03,796
prove lots of theorems. We'll be able to
establish a lot of performance guarantees

194
00:14:03,796 --> 00:14:08,223
for fundamental algorithms but at the same
time we'll have good predictive power what

195
00:14:08,223 --> 00:14:12,186
the theory advocates will in fact be
algorithms that are known to be best in

196
00:14:12,186 --> 00:14:16,080
practice. So, the final explanation I owe
you, is, what do I mean by, the running

197
00:14:16,080 --> 00:14:19,863
time grows slowly, with respect to the
input size? Well, the answer depends a

198
00:14:19,863 --> 00:14:23,697
little bit on the context, but, for almost
all of the problems we're going to

199
00:14:23,697 --> 00:14:27,733
discuss, the holy grail will be to have
what's called a linear time algorithm, an

200
00:14:27,733 --> 00:14:31,718
algorithm whose number of instructions
grows proportional to the input size. So,

201
00:14:31,718 --> 00:14:35,804
we won't always be able to achieve linear
time, but that's, in some sense, the best

202
00:14:35,804 --> 00:14:39,789
case scenario. Notice, linear time is even
better than what we achieved with our

203
00:14:39,789 --> 00:14:43,875
merge short algorithm for sorting. Merge
short runs a little bit superlinear, it's

204
00:14:43,875 --> 00:14:47,729
n times log n, running as the input size.
If possible, we. To be linear time. It's

205
00:14:47,729 --> 00:14:51,588
not always gonna be possible, but that is
what we will aspire toward. For most of

206
00:14:51,588 --> 00:14:56,036
the problems we'll discuss in this course.
Looking ahead, the next series of videos

207
00:14:56,036 --> 00:14:59,986
is going to have two goals. First of all,
on the analysis side, I'll describe

208
00:14:59,986 --> 00:15:03,989
formally what I mean by asymptotic
running time. I'll introduce "Big Oh"

209
00:15:03,989 --> 00:15:07,992
notation and its variants, explain its
mathematical definitions, and give a

210
00:15:07,992 --> 00:15:12,047
number of examples. On the design side,
we'll get more experience applying the

211
00:15:12,047 --> 00:15:14,102
divide and conquer paradigm to further problems.  See you then.
