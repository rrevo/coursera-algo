1
00:00:00,012 --> 00:00:02,634
This video is a segue between bad news 
and good news. 

2
00:00:02,634 --> 00:00:06,710
The bad news which we've now discussed is 
NP-Completeness; the fact that there are 

3
00:00:06,710 --> 00:00:10,574
computationally intractable problems out 
there in the world, in fact they're 

4
00:00:10,574 --> 00:00:14,462
fairly ubiquitous and you're likely to 
encounter them in your own project. 

5
00:00:14,462 --> 00:00:18,243
The good news is that NP-Completeness is 
hardly a death sentence. 

6
00:00:18,243 --> 00:00:22,728
Indeed, our algorithmic toolbox is now 
rich enough to provide many different 

7
00:00:22,728 --> 00:00:25,572
strategies toward coping with 
NP-Completeness. 

8
00:00:25,572 --> 00:00:28,064
NP problems. 
So, suppose you had identified a 

9
00:00:28,064 --> 00:00:32,401
computational problem on which on which 
the success of [UNKNOWN] company rests. 

10
00:00:32,401 --> 00:00:36,670
May be you have spent the last several 
weeks throwing the kitchen sink added/g. 

11
00:00:36,670 --> 00:00:40,737
All the algorithm designed for [UNKNOWN], 
all the data structures, all the 

12
00:00:40,737 --> 00:00:44,223
primitives nothing works. 
Finally you decide to try to prove the 

13
00:00:44,223 --> 00:00:49,071
problem is NP-Complete And you succeed. 
Now you have an explanation for why your 

14
00:00:49,071 --> 00:00:53,332
weeks of effort have come to not. 
But that doesn't change the fact that 

15
00:00:53,332 --> 00:00:57,049
this is the problem that governs the 
success of your project. 

16
00:00:57,049 --> 00:01:01,725
What should you do? Well, the good news 
is NP-Completeness is certainly not a 

17
00:01:01,725 --> 00:01:04,970
death sentence. 
There are people solving, or at least 

18
00:01:04,970 --> 00:01:08,561
approximately solving, NP-Complete 
problems all the time. 

19
00:01:08,561 --> 00:01:13,135
However, knowing that your problem is 
NP-Complete does tell you where to set 

20
00:01:13,135 --> 00:01:16,522
your expectations. 
You should not expect some general 

21
00:01:16,522 --> 00:01:21,093
Purpose, super fast algorithm, like we 
have for other computational problems. 

22
00:01:21,093 --> 00:01:23,926
Like say, sorting or single sort shortest 
paths. 

23
00:01:23,926 --> 00:01:28,384
Unless you're dealing with unusually 
small or well structured inputs, you're 

24
00:01:28,384 --> 00:01:32,602
going to have to work pretty hard to 
solve this problem, and also possibly 

25
00:01:32,602 --> 00:01:36,361
make some compromises. 
[INAUDIBLE] The rest of this course is 

26
00:01:36,361 --> 00:01:40,740
devoted to strategies for solving or 
approximately solving NP-Complete 

27
00:01:40,740 --> 00:01:43,787
problems. 
In the rest of this video, I'll give you 

28
00:01:43,787 --> 00:01:48,542
an orientation for what those strategies 
are, and what you can expect to come. 

29
00:01:48,542 --> 00:01:53,004
So as usual, I'm going to focus here on 
general purpose strategies, that cut 

30
00:01:53,004 --> 00:01:56,042
across multiple application domains. 
Means. 

31
00:01:56,042 --> 00:02:00,072
As usual, these general principles should 
just be a starting point. 

32
00:02:00,072 --> 00:02:04,517
You should take them and run with them, 
admitting them with whatever domain 

33
00:02:04,517 --> 00:02:08,482
expertise you have in this specific 
problem that you need to solve. 

34
00:02:08,482 --> 00:02:13,722
The first strategy is to focus on 
computationally tractable special cases 

35
00:02:13,722 --> 00:02:17,727
of an NP-Complete problem. 
Relatedgee, relatedly you want to think 

36
00:02:17,727 --> 00:02:21,056
about what's special about your domain. 
[INAUDIBLE]. 

37
00:02:21,056 --> 00:02:25,136
About the data sets that you're working 
with, and try to understand if there's 

38
00:02:25,136 --> 00:02:28,293
special structure which can be exploited 
in your algorithm. 

39
00:02:28,293 --> 00:02:32,552
Let me point out that we've already done 
this in a couple cases in this course. 

40
00:02:32,552 --> 00:02:36,127
The first example we saw concerned the 
weighted independent set. 

41
00:02:36,127 --> 00:02:40,112
So we studied this problem on path 
graphs, but the computational problem 

42
00:02:40,112 --> 00:02:44,292
makes perfect sense in general graphs. 
The general problem is, I give you as 

43
00:02:44,292 --> 00:02:48,027
input an undirected graph. 
Every vertex has a weight, and I want the 

44
00:02:48,027 --> 00:02:51,552
maximum weight subset of vertices that's 
an independent set. 

45
00:02:51,552 --> 00:02:55,703
And remember in an independent set you 
are forbidden to to take any 2 vertices 

46
00:02:55,703 --> 00:02:58,717
that are neighbors. 
So in an independent set none of the 

47
00:02:58,717 --> 00:03:01,908
pairs of vertices that you've picked are 
joined by an edge. 

48
00:03:01,908 --> 00:03:05,841
In general graphs the weighted 
independent set problem is NP-Complete so 

49
00:03:05,841 --> 00:03:09,445
we certainly don't expect it to have an 
polynomial time algorithm. 

50
00:03:09,445 --> 00:03:13,578
But in the special case where the graph 
is a path, as we saw, there's a linear 

51
00:03:13,578 --> 00:03:17,958
time dynamic programming algorithm that 
exactly solves the weighted independent 

52
00:03:17,958 --> 00:03:20,956
set. 
So path graphs form a special case of the 

53
00:03:20,956 --> 00:03:25,402
weighted independent set problem that's 
compucation retractable solvable in 

54
00:03:25,402 --> 00:03:29,570
polynomial or even linear time. 
In fact the frontier of tractability can 

55
00:03:29,570 --> 00:03:33,561
be pushed well beyond path graphs. 
On the homework, I ask you to think 

56
00:03:33,561 --> 00:03:38,382
through the case of graphs that are trees 
and notice that you can still do dynamic 

57
00:03:38,382 --> 00:03:41,933
Programming efficiently, to compute 
weighted independent sets in trees. 

58
00:03:41,933 --> 00:03:45,484
You can even get computationally 
efficient algorithms for a broader class 

59
00:03:45,484 --> 00:03:47,789
of graphs, known as bounded tree-width 
graphs. 

60
00:03:47,789 --> 00:03:51,517
So the definition of that's a little 
outside the scope of this course, but you 

61
00:03:51,517 --> 00:03:54,988
can go even beyond trees. 
So the second example follows from my 

62
00:03:54,988 --> 00:03:58,177
dynamic programming algorithm for the 
knapsack problem. 

63
00:03:58,177 --> 00:04:02,309
So, we discussed that running time. 
And we explained why it's exponential 

64
00:04:02,309 --> 00:04:04,547
time. 
If the running time of our dynamic 

65
00:04:04,547 --> 00:04:08,851
programming napsac algorithm is n, the 
number of items, times capital w, the 

66
00:04:08,851 --> 00:04:11,957
napsac capacity. 
And because it only takes log w bits to 

67
00:04:11,957 --> 00:04:16,938
specify the capacity, capital w, we don't 
Call that a polynomial time algorithm. 

68
00:04:16,938 --> 00:04:21,601
But, imagine you only have to solve a 
knapsack instance where the capacity is 

69
00:04:21,601 --> 00:04:25,736
not too big, maybe even say the capacity, 
capital W, is big O event. 

70
00:04:25,736 --> 00:04:30,615
And you definitely will see knapsack 
instances in practice, which posses That 

71
00:04:30,615 --> 00:04:35,094
property, well then our dynamic 
programming algorithm just runs in time o 

72
00:04:35,094 --> 00:04:39,305
of n squared and that's a bona fide 
polynomial time algorithm for this 

73
00:04:39,305 --> 00:04:41,917
special case of a small knapsack 
capacity. 

74
00:04:41,917 --> 00:04:45,977
So next let me mention a couple of 
examples we're going to see in the 

75
00:04:45,977 --> 00:04:49,742
forthcoming videos. 
The 1st 1 is going to concern the 2SAT 

76
00:04:49,742 --> 00:04:52,812
problem. 
The 2SAT is a type of constraint 

77
00:04:52,812 --> 00:04:56,192
satisfaction problem. 
Remember in a constraint satisfaction 

78
00:04:56,192 --> 00:05:00,047
problem you have a bunch of variables, 
each one gets assigned a value. 

79
00:05:00,047 --> 00:05:04,192
So the simplest case is the Bullion case 
where each value can be zero or one, 

80
00:05:04,192 --> 00:05:07,037
false or true. 
And then you have a bunch of clauses, 

81
00:05:07,037 --> 00:05:11,449
which specify the permitted joint Values 
of a collection of variables. 

82
00:05:11,449 --> 00:05:16,601
The 2 and 2 set refers to the fact that 
each constraint involves the joint values 

83
00:05:16,601 --> 00:05:20,792
of only a pair of variables. 
So a conconical constraint in a 2 set 

84
00:05:20,792 --> 00:05:25,952
instance is going to for 2 variables 
specify 3 joint assignments that allowed. 

85
00:05:25,952 --> 00:05:29,638
And one that's forbidden. 
So, for example, maybe it will say, oh 

86
00:05:29,638 --> 00:05:32,989
for variables x3 and x7, it's okay to set 
them both to true. 

87
00:05:32,989 --> 00:05:36,961
It's okay to set them both to false. 
It's okay to set 3 to true, and 7 to 

88
00:05:36,961 --> 00:05:39,679
false. 
But it's forbidden to set 3 to false, and 

89
00:05:39,679 --> 00:05:42,583
7 to true. 
So that's a canonical constraint in the 2 

90
00:05:42,583 --> 00:05:45,574
sat instance. 
3 sat, it's the same thing, except the 

91
00:05:45,574 --> 00:05:49,073
constraints involve the join values to a 
triple of variables. 

92
00:05:49,073 --> 00:05:52,132
And it's going to forbid 1 out of the 8 
possibilities. 

93
00:05:52,132 --> 00:05:56,179
Now the 3 set problems a canonical MP 
complete problem. 

94
00:05:56,179 --> 00:06:02,187
That was really singled out by Cooke and 
Levin as being sufficiently expressive to 

95
00:06:02,187 --> 00:06:06,552
encode all problems in MP, but if each 
constraint has size. 

96
00:06:06,552 --> 00:06:10,417
Only 2 then as we'll see the problem 
becomes polynomial time solvable. 

97
00:06:10,417 --> 00:06:14,572
There's a couple ways of proving that. 
We're going to talk about a local search 

98
00:06:14,572 --> 00:06:18,557
algorithm that checks whether or not 
there is indeed an assignment to the 

99
00:06:18,557 --> 00:06:22,572
variables that similtaneously satisfies 
all of the given constraints. 

100
00:06:22,572 --> 00:06:27,146
So the final example we'll discuss in 
more detail later but just very briefly 

101
00:06:27,146 --> 00:06:29,920
we're going to discuss the vertex cover 
problem. 

102
00:06:29,920 --> 00:06:34,057
This is a graph problem and a vertex 
cover is just the compliment of an 

103
00:06:34,057 --> 00:06:37,475
independent set. 
So while an independent set cannot take 

104
00:06:37,475 --> 00:06:42,190
two vertices from the same edge, in the 
vertex cover problem you have to take at 

105
00:06:42,190 --> 00:06:46,709
least One vertex from every single edge, 
and then what you want is you want the 

106
00:06:46,709 --> 00:06:49,945
vertex cover that minimizes the sum of 
the vertex rates. 

107
00:06:49,945 --> 00:06:54,264
Yet again this is an NP-Complete problem 
in general but we're going to focus on 

108
00:06:54,264 --> 00:06:57,349
the special case where the optimal 
solution is small. 

109
00:06:57,349 --> 00:07:01,982
That is we're going to focus on graphs 
where there's a small number of vertices. 

110
00:07:01,982 --> 00:07:06,310
Such that every single edge has at least 
one endpoint in that small set. 

111
00:07:06,310 --> 00:07:10,864
And, we'll see that for that special 
case, using a smart kind of exhaustive 

112
00:07:10,864 --> 00:07:15,252
search will actually be able to solve the 
problem in polynomial time. 

113
00:07:15,252 --> 00:07:19,571
So, let me reiterate that these tractable 
special cases are meant primarily to be 

114
00:07:19,571 --> 00:07:23,898
building blocks upon which you can draw 
in a possibly more sophisticated approach 

115
00:07:23,898 --> 00:07:27,322
to your NP-complete problem. 
So, just to make this a little more 

116
00:07:27,322 --> 00:07:31,202
concrete, let me just kind of dream up 
one scenario to let you know what I'm 

117
00:07:31,202 --> 00:07:34,112
thinking about. 
Imagine, for example, that you have a 

118
00:07:34,112 --> 00:07:38,265
project where, unfortunately, it's not 
really 2SAT that you're confronting. 

119
00:07:38,265 --> 00:07:42,255
It's actually A 3SAT instance. 
So you're feeling kind of bummed, 3SATs 

120
00:07:42,255 --> 00:07:45,403
have been complete and maybe you have 
1,000 variables. 

121
00:07:45,403 --> 00:07:49,747
And certainly you can't do brute-force 
search over 2 to 1,000 possible ways of 

122
00:07:49,747 --> 00:07:54,431
assigning values to your 1,000 variables. 
But maybe the good news is, because you 

123
00:07:54,431 --> 00:07:58,398
have domain expertise because you 
understand this problem instance. 

124
00:07:58,398 --> 00:08:01,855
You know that yeah. 
There's a 1000 variables but there's 

125
00:08:01,855 --> 00:08:05,481
really 20 that are crucial. 
You have the feeling that all of the 

126
00:08:05,481 --> 00:08:09,926
action basically is boiling down to how 
these 20 core variables get assigned. 

127
00:08:09,926 --> 00:08:14,333
Well now maybe you can mix together some 
brute-force search with some of these 

128
00:08:14,333 --> 00:08:18,992
tractable special cases, for example you 
could enumerate overall 2 to the 20 ways 

129
00:08:18,992 --> 00:08:22,202
of assigning values to this core set of 
20 variables. 

130
00:08:22,202 --> 00:08:25,167
To the 20 is roughly a million. 
That's not so bad. 

131
00:08:25,167 --> 00:08:29,618
And now, what you're going to do is, for 
each of these million scenarios, you 

132
00:08:29,618 --> 00:08:34,547
check whether there isn't a way to extend 
that tentative assignment of 20 values to 

133
00:08:34,547 --> 00:08:39,105
the 20 variables, to the other 980 
variables, so that all of the constraints 

134
00:08:39,105 --> 00:08:42,482
get satisfied. 
The original problem is solvable, if and 

135
00:08:42,482 --> 00:08:46,251
only if there exists. 
A way of assigning values to these 20 

136
00:08:46,251 --> 00:08:49,355
variables that successfully extends the 
other 980. 

137
00:08:49,355 --> 00:08:53,888
Now, because these are the crucial 
variables, and it's where all the action 

138
00:08:53,888 --> 00:08:58,075
is, maybe as soon as you assign all of 
them 0's and 1's the residual SAT 

139
00:08:58,075 --> 00:09:01,240
instance is tracked. 
For example maybe it just becomes a 

140
00:09:01,240 --> 00:09:04,700
simple 2SAT instance and then you can 
solve it in polynomial time. 

141
00:09:04,700 --> 00:09:08,691
So this gives you a hybrid approach, 
approach, brute-force search at the top 

142
00:09:08,691 --> 00:09:12,917
level, tractable special cases for each 
guess of assignments of the 20 variables 

143
00:09:12,917 --> 00:09:15,604
and you're off to the races. 
And I hope it's clear. 

144
00:09:15,604 --> 00:09:19,543
I mean this as just one possible way that 
you might combine the various building 

145
00:09:19,543 --> 00:09:22,896
blocks for developing into a more 
elaborate approach to tackling an 

146
00:09:22,896 --> 00:09:25,846
NP-Complete problem. 
And that's generally what they take. 

147
00:09:25,846 --> 00:09:29,814
They take a fairly elaborate approach 
because after all, they are NP-Complete, 

148
00:09:29,814 --> 00:09:32,246
you've got to respect them. 
Expect that. 

149
00:09:32,246 --> 00:09:37,244
So with that digression complete, let me 
mention what are the other 2 strategies 

150
00:09:37,244 --> 00:09:40,323
we're going to be exploring in the 
lectures to come. 

151
00:09:40,323 --> 00:09:45,070
So, the second strategy, which is 
certainly one very common in practice, is 

152
00:09:45,070 --> 00:09:48,820
to resort to heuristics. 
That is to algorithms which are not 

153
00:09:48,820 --> 00:09:52,675
guaranteed [INAUDIBLE] to be correct. 
We haven't really seen examples of 

154
00:09:52,675 --> 00:09:56,552
heuristics in the course thus far. 
those of you that are alumni of part 1. 

155
00:09:56,552 --> 00:10:00,337
Perhaps we could classify [INAUDIBLE] 
randomized minimum cut algorithm as a 

156
00:10:00,337 --> 00:10:02,506
heuristic. 
'Cuz it did have a small failure 

157
00:10:02,506 --> 00:10:05,394
probability of, of failing to find, the 
minimum cut. 

158
00:10:05,394 --> 00:10:08,930
But rather, I'm going to focus on some 
examples in the upcoming lectures. 

159
00:10:08,930 --> 00:10:12,257
I'm going to use the [INAUDIBLE]. 
Knapsack problem as a case study. 

160
00:10:12,257 --> 00:10:16,292
And what we'll see is that our toolbox, 
which contains various algorithm design 

161
00:10:16,292 --> 00:10:18,637
paradigms. 
It's useful not just for designing 

162
00:10:18,637 --> 00:10:21,447
correct algorithms. 
But it's also useful for designing 

163
00:10:21,447 --> 00:10:24,032
heuristics. 
So, in particular, we'll get a pretty 

164
00:10:24,032 --> 00:10:27,772
good algorithm for the knapsack problem 
using the Greedy Algorithm design 

165
00:10:27,772 --> 00:10:29,742
paradigm. 
And we'll get an excellent. 

166
00:10:29,742 --> 00:10:34,408
Heuristic for knapsack using the dynamic 
programming algorithm designed paradigm. 

167
00:10:34,408 --> 00:10:38,130
The final strategy is for situations 
where you are unwilling to relax 

168
00:10:38,130 --> 00:10:40,699
correctness unwilling to consider 
heuristics. 

169
00:10:40,699 --> 00:10:44,732
Now first one can be NP complete problem 
if your hours going to be correct your 

170
00:10:44,732 --> 00:10:48,327
not, you don't expect to run on 
polynomial time but there are still 

171
00:10:48,327 --> 00:10:52,841
opportunities to have algorithms that. 
Well exponential time in the worst case 

172
00:10:52,841 --> 00:10:55,129
are smarter than naive brute-force 
search. 

173
00:10:55,129 --> 00:10:58,810
So we have in fact already seen on 
example that can be interpreted as a 

174
00:10:58,810 --> 00:11:02,333
implementation of the strategy, that's 
for the knapsack problem. 

175
00:11:02,333 --> 00:11:06,080
So in the knapsack problem naive 
brute-force search would just run over 

176
00:11:06,080 --> 00:11:10,134
all possible subsets of the items, it 
would check if the subsets of items fit 

177
00:11:10,134 --> 00:11:14,271
in the knapsack if it does remembers the 
value and then it just returns the 

178
00:11:14,271 --> 00:11:19,442
feasible solution with maximum value. 
That has time proportional to 2^n, where 

179
00:11:19,442 --> 00:11:22,816
N is the number of items. 
Our dynamic programming algorithm has 

180
00:11:22,816 --> 00:11:26,953
running time N times times capital W. 
Now, of course, cap this is no better 

181
00:11:26,953 --> 00:11:29,598
than 2^n, if the knapsack capacity is 
huge, 

182
00:11:29,598 --> 00:11:34,200
it is itself 2^n, But as we argued, if W 
is smaller, this algorithm is going to be 

183
00:11:34,200 --> 00:11:36,621
faster. 
And also, as you learned on the third 

184
00:11:36,621 --> 00:11:40,890
programming assignment, sometimes even 
though W is big, dynamic programming is 

185
00:11:40,890 --> 00:11:43,692
going to beat the crap out of brute-force 
search. 

186
00:11:43,692 --> 00:11:47,955
[SOUND] So I'll show you a couple of 
other examples we'll talk about the 

187
00:11:47,955 --> 00:11:51,927
traveling salesman problem where naive 
brute-force search would take roughly in 

188
00:11:51,927 --> 00:11:54,470
factorial time. 
Where n is the number of vertices. 

189
00:11:54,470 --> 00:11:58,201
We'll give an alternative dynamic 
programming based solution which runs in 

190
00:11:58,201 --> 00:12:00,952
time only 2^n which is much better than 
in factorial. 

191
00:12:00,952 --> 00:12:05,354
The third example, which we'll cover in a 
forthcoming video, we already alluded to 

192
00:12:05,354 --> 00:12:08,637
briefly on the last slide. 
It's for the vertex cover problem. 

193
00:12:08,637 --> 00:12:12,741
So this is where you're given a graph. 
If your vertex has a weight and you want 

194
00:12:12,741 --> 00:12:16,172
the minimum weight subset of vertices 
that includes at least. 

195
00:12:16,172 --> 00:12:20,259
One endpoint from every edge. 
We're going to consider the version of 

196
00:12:20,259 --> 00:12:24,594
the problem where you want to check 
whether or not it's possible to have a 

197
00:12:24,594 --> 00:12:29,259
vertex cover that uses only K vertices. 
Whether or not there exists K vertices 

198
00:12:29,259 --> 00:12:32,347
that includes one endpoint at least from 
each edge. 

199
00:12:32,347 --> 00:12:36,732
The naiive root brute-force search will 
run in time, end the K, which gets 

200
00:12:36,732 --> 00:12:41,132
absurd, even when K is quite small. 
But we're going to show that there's a 

201
00:12:41,132 --> 00:12:45,472
smarter algorithm, which is still 
exponential in k, but runs in time only 

202
00:12:45,472 --> 00:12:46,052
2^k times the size of the graph. 

