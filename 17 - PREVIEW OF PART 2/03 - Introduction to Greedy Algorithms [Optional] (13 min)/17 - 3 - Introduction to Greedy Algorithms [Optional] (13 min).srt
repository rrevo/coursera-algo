1
00:00:00,012 --> 00:00:03,618
Today, we're going to embark on the 
discussion of a new algorithm design 

2
00:00:03,618 --> 00:00:07,290
paradigms namely that of designing and 
analyzing Greedy Algorithms. 

3
00:00:07,290 --> 00:00:11,402
So to put this study of greedy algorithms 
in a little bit of context let's just 

4
00:00:11,402 --> 00:00:15,721
zoom out, let's both review some of the 
algorithm designs paradigms that we've 

5
00:00:15,721 --> 00:00:19,962
already seen as well look forward to some 
that we're going to learn later on. 

6
00:00:19,962 --> 00:00:23,254
in this course. 
So it's sort of a sad fact of life, that 

7
00:00:23,254 --> 00:00:27,752
in algorithm design there's no one silver 
bullet, there's no magic potion that's 

8
00:00:27,752 --> 00:00:30,331
the cure for all your computational 
problems. 

9
00:00:30,331 --> 00:00:34,588
So instead, the best we can do, in, in 
the focus of these courses is to discuss 

10
00:00:34,588 --> 00:00:38,638
general techniques. 
That apply to lots of different problems 

11
00:00:38,638 --> 00:00:43,050
that arise in lots of different domains. 
So that's what I mean by algorithm design 

12
00:00:43,050 --> 00:00:47,101
paradigms, high level problem solving 
strategies that cut across multiple 

13
00:00:47,101 --> 00:00:49,722
applications. 
So let's look at some examples. 

14
00:00:49,722 --> 00:00:53,427
Back in part one, we started with the 
divide and conquer algorithm design 

15
00:00:53,427 --> 00:00:56,017
paradigm. 
A conical example of that paradigm being 

16
00:00:56,017 --> 00:00:59,287
the merge sort algorithm. 
So remember in divide and conquer what 

17
00:00:59,287 --> 00:01:03,152
you do is you take your problem, you 
break it into smaller sub-problems, you 

18
00:01:03,152 --> 00:01:06,917
solve the sub-problems recursively and 
then you combine the results into a 

19
00:01:06,917 --> 00:01:10,597
solution to the original problem. 
Like how in merge sort you recursively 

20
00:01:10,597 --> 00:01:14,437
sort two sub-arrays and then merge the 
results to get a sorted version of the 

21
00:01:14,437 --> 00:01:18,087
original input array. 
Another paradigm that we touched on in 

22
00:01:18,087 --> 00:01:22,182
part 1 although we didn't discuss it any 
where near as thoroughly is that of 

23
00:01:22,182 --> 00:01:25,747
randomized algorithms. 
So the idea that you can have codes, flip 

24
00:01:25,747 --> 00:01:30,022
coins that is make random choices is 
inside the code itself often this leads 

25
00:01:30,022 --> 00:01:33,797
to simpler, more practical or more. 
Elegant algorithms. 

26
00:01:33,797 --> 00:01:36,852
A kind of that application here is the 
quick sort algorithm. 

27
00:01:36,852 --> 00:01:40,522
using a random pivot element. 
But we also saw applications, for 

28
00:01:40,522 --> 00:01:44,717
example, to the design of hash functions. 
So the next measure paradigm we're going 

29
00:01:44,717 --> 00:01:48,862
to discuss is that of Greedy Algorithms. 
So these are algorithms that iteratively 

30
00:01:48,862 --> 00:01:52,425
make myopic decisions. 
In fact, we've already seen an example of 

31
00:01:52,425 --> 00:01:56,532
a greedy algorithm in part 1, namely 
Dijkstra's shortest path algoithm. 

32
00:01:56,532 --> 00:02:00,565
And then the final paradigm, we're 
going to discuss in this class, is that 

33
00:02:00,565 --> 00:02:05,280
of dynamic programming, a very powerful 
paradigm, which solves in particular, two 

34
00:02:05,280 --> 00:02:09,871
of the motivating questions we saw 
earlier namely sequence alignment and 

35
00:02:09,871 --> 00:02:14,071
distributed shortest paths. 
So what is a greedy algorithms anyways? 

36
00:02:14,071 --> 00:02:18,301
Well, to be honest, I'm not going to 
offer you a formal definition, in fact, 

37
00:02:18,301 --> 00:02:22,809
much blood and ink has been spilled over 
which algorithms precise here, greedy 

38
00:02:22,809 --> 00:02:26,020
algorithms, but I'll give you an informal 
description. 

39
00:02:26,020 --> 00:02:30,182
A sort of, rule of thumb for what greedy 
algorithms usually look like. 

40
00:02:30,182 --> 00:02:33,811
Generally speaking, what a Greedy 
Algorithm does is make a sequence of 

41
00:02:33,811 --> 00:02:36,612
decisions. 
With each decision being made myopically. 

42
00:02:36,612 --> 00:02:39,009
That is, it seems like a good idea at the 
time. 

43
00:02:39,009 --> 00:02:41,829
And then you hope that everything works 
out at the end. 

44
00:02:41,829 --> 00:02:45,284
The best way to get a feel for Greedy 
Algorithms is to see examples. 

45
00:02:45,284 --> 00:02:48,412
And the upcoming lectures will give you a 
number of them. 

46
00:02:48,412 --> 00:02:50,797
Algorithm. 
But I want to point out we've actually 

47
00:02:50,797 --> 00:02:54,412
already seen an example of a greedy 
algorithm in part one of this course, 

48
00:02:54,412 --> 00:02:56,614
mainly Dijkstra's shortest path 
algorithm. 

49
00:02:56,614 --> 00:03:00,711
So, in what sense is Dijkstra's algorithm 
a greedy algorithm? Well, if you recall 

50
00:03:00,711 --> 00:03:04,758
the pseudo code for Dijkstra's algorithm, 
you'll recall this one main wild loop. 

51
00:03:04,758 --> 00:03:08,809
And the algorithm process is exactly one 
new destination vertex in each iteration 

52
00:03:08,809 --> 00:03:11,435
of this wild loop. 
So there's exactly n-1 iterations 

53
00:03:11,435 --> 00:03:13,752
overall. 
Where N is the number of vertices. 

54
00:03:13,752 --> 00:03:17,482
So the algorithm only gets one shot to 
compute the shortest path to a given 

55
00:03:17,482 --> 00:03:20,017
destination. 
It never goes back and re-visits the 

56
00:03:20,017 --> 00:03:22,657
decision. 
In that sense, the decisions are myopic, 

57
00:03:22,657 --> 00:03:25,412
irrevocable. 
and that's the sense in which Dijkstra's 

58
00:03:25,412 --> 00:03:28,432
algorithm is greedy. 
So let me pause for a moment to discuss 

59
00:03:28,432 --> 00:03:30,997
the greedy algorithm design paradigm, 
generally. 

60
00:03:30,997 --> 00:03:34,757
Probably, this discussion will seem a 
little abstract, so I recommend you 

61
00:03:34,757 --> 00:03:38,289
revisit this discussion on the slide. 
After we've seen a few examples. 

62
00:03:38,289 --> 00:03:40,525
At that point, I think it will really hit 
home. 

63
00:03:40,525 --> 00:03:43,731
So let me proceed by comparing it and 
contrasting it to the per a diem we've 

64
00:03:43,731 --> 00:03:46,515
already studied in depth, that of divide 
and conquer algorithm. 

65
00:03:46,515 --> 00:03:49,742
So you'll recall what you do in the 
divide and conquer algorithm what you do 

66
00:03:49,742 --> 00:03:53,102
is you break the problem into sub 
problems, so maybe to take an imput array 

67
00:03:53,102 --> 00:03:56,868
And you split it into 2 subarrays. 
Then you solve the smaller subproblems 

68
00:03:56,868 --> 00:03:59,582
recursively. 
And then you combine the results of the 

69
00:03:59,582 --> 00:04:02,215
subproblems into a solution to the 
original input. 

70
00:04:02,215 --> 00:04:05,539
So, the Greedy paradigm is quite 
different in several respects. 

71
00:04:05,539 --> 00:04:09,666
First, both a strength and a weakness of 
the Greedy Algorithm design paradigm is 

72
00:04:09,666 --> 00:04:12,052
just how easy it is to apply. 
So, it's often. 

73
00:04:12,052 --> 00:04:15,564
Quite easy to come up with plausible 
greedy algorithms for a problem, even 

74
00:04:15,564 --> 00:04:17,907
multiple different plausible greedy 
algorithms. 

75
00:04:17,907 --> 00:04:21,241
I think that's a point of contrast with 
divide and conquer algorithms. 

76
00:04:21,241 --> 00:04:24,728
Often it's tricky to come up with a 
plausible divide and conquer algorithm 

77
00:04:24,728 --> 00:04:28,316
and usually you have this eureka moment, 
where you finally figure out how to 

78
00:04:28,316 --> 00:04:32,165
decompose the problem in the right way. 
And once you have the eureka moment, 

79
00:04:32,165 --> 00:04:35,506
you're good to go. 
So secondly, I'm happy to report that 

80
00:04:35,506 --> 00:04:39,057
analyzing running time of greedy 
algorithms will generally be much easier 

81
00:04:39,057 --> 00:04:41,362
that it was with divide and conquer 
algorithms. 

82
00:04:41,362 --> 00:04:45,248
For divide and conquer algorithms, it was 
really unclear whether they were fast or 

83
00:04:45,248 --> 00:04:48,874
slow because we had to understand the 
running time over multiple levels of 

84
00:04:48,874 --> 00:04:51,498
recursion. 
On the one hand, problem size was getting 

85
00:04:51,498 --> 00:04:54,837
smaller, on the other hand the number of 
sub problems was proliferating. 

86
00:04:54,837 --> 00:04:58,152
So we had to work hard, we developed 
these powerful tools like the master 

87
00:04:58,152 --> 00:05:01,613
method and some other techniques for 
figuring out just how fast an algorithm 

88
00:05:01,613 --> 00:05:04,216
like merge short runs. 
Or just how fast an algorithm like 

89
00:05:04,216 --> 00:05:07,687
Strassen's fast matrix multiplication. 
Application algorithm runs. 

90
00:05:07,687 --> 00:05:11,007
In contrast, with greedy algorithms, 
it'll often be a one liner. 

91
00:05:11,007 --> 00:05:14,927
Often it'll be clear that the work is 
dominated by, say, a sorting subroutine 

92
00:05:14,927 --> 00:05:19,162
and, of course, we all know that sorting 
takes n log n time if you use a sensible 

93
00:05:19,162 --> 00:05:21,987
algorithm for it. 
Now, the catch, and this is the third 

94
00:05:21,987 --> 00:05:25,692
point of comparison, is we're generally 
going to have to work much harder to 

95
00:05:25,692 --> 00:05:28,208
understand correctness issues of greed 
outcomes. 

96
00:05:28,208 --> 00:05:31,861
For divide and conquer algorithms we 
didn't talk much about correctness, it 

97
00:05:31,861 --> 00:05:34,609
was generally a pretty straightforward 
induction proof. 

98
00:05:34,609 --> 00:05:38,206
You can view the lectures on quick short 
if you want an example of 1 of those 

99
00:05:38,206 --> 00:05:42,146
canonical inductive correctness proofs. 
But the game totally changes with greedy 

100
00:05:42,146 --> 00:05:44,528
algorithms. 
In fact, given a greedy algorithm we 

101
00:05:44,528 --> 00:05:46,872
often won't even have very good 
intuition. 

102
00:05:46,872 --> 00:05:51,493
For whether or not they are correct, let 
alone how to prove they're correct. 

103
00:05:51,493 --> 00:05:55,729
So even with a correct algorithm it's 
often hard to figure out why it's 

104
00:05:55,729 --> 00:05:58,550
correct. 
And in fact, if you remember only one 

105
00:05:58,550 --> 00:06:02,772
thing From all of this greedy algorithm 
discussion many years from now I hope one 

106
00:06:02,772 --> 00:06:05,687
key thing you remember is that they're 
often not correct. 

107
00:06:05,687 --> 00:06:09,477
Often especially if it's one that you 
proposed yourself, which you're very 

108
00:06:09,477 --> 00:06:12,552
biased in favor of. 
You will think the algorithm, the greedy 

109
00:06:12,552 --> 00:06:15,192
algorithm must be correct because it's so 
natural. 

110
00:06:15,192 --> 00:06:17,737
But many of them are not. 
So keep that in mind. 

111
00:06:17,737 --> 00:06:21,877
So to give you some immediate practice 
with the ubiquitous incorrectness of 

112
00:06:21,877 --> 00:06:25,392
natural greedy algorithms. 
Let's review a point that we already 

113
00:06:25,392 --> 00:06:28,967
covered in part 1 of this class 
concerning Dijkstra's algorithm. 

114
00:06:28,967 --> 00:06:32,857
Now in part 1 you made a deal about a 
justly famous algorithm Dijkstra's 

115
00:06:32,857 --> 00:06:36,697
shortest path algorithm is. 
It runs blazingly fast and it computes 

116
00:06:36,697 --> 00:06:40,192
all of the shortest path. 
What else do you want? Well, remember 

117
00:06:40,192 --> 00:06:43,997
there is an assumption when we proved the 
Dykes algorithm is correct. 

118
00:06:43,997 --> 00:06:48,297
We assumed that every edge of the given 
network has a non-negative length. 

119
00:06:48,297 --> 00:06:52,633
We did not allow Negative edge lengths. 
And as we discussed in part one, you know 

120
00:06:52,633 --> 00:06:56,239
for many applications you only care about 
non negative edge lengths. 

121
00:06:56,239 --> 00:06:59,788
But there are applications where you do 
want negative edge lengths. 

122
00:06:59,788 --> 00:07:03,622
So let's review on this quiz why 
Dijkstra's is actually incorrect despite 

123
00:07:03,622 --> 00:07:06,601
being so natural. 
it's incorrect when edges can have 

124
00:07:06,601 --> 00:07:09,733
negative length. 
So I've drawn in green a very simple 

125
00:07:09,733 --> 00:07:14,336
shortest path network with 3 edges and 
I've antitated the edges with there 

126
00:07:14,336 --> 00:07:17,302
length. 
You'll notice 1 of the edges does have a 

127
00:07:17,302 --> 00:07:20,594
negative length. 
The edge from V to W with length -2. 

128
00:07:20,594 --> 00:07:25,561
So the question is consider the source 
vertex s, and the destination vertex w. 

129
00:07:25,561 --> 00:07:30,006
And the question is, what is the shortest 
path distance computed by Dykstra's 

130
00:07:30,006 --> 00:07:34,626
Algorithm? And you may have to go and 
review, just the pseudo code in part 1, 

131
00:07:34,626 --> 00:07:38,012
or in, on the web, to answer that part of 
the question. 

132
00:07:38,012 --> 00:07:42,594
And then, what is, in fact, the actual 
shortest path distance, from s to w? 

133
00:07:42,594 --> 00:07:47,000
Where, as usual, the length of a path is 
just the sum of the links of the edges 

134
00:07:47,000 --> 00:07:48,432
in. 
In the path. 

135
00:07:48,432 --> 00:07:53,493
Alright, so the correct answer is d. 
So let's start with the second part of 

136
00:07:53,493 --> 00:07:57,112
the question. 
What is the actual length of a shortest 

137
00:07:57,112 --> 00:08:01,413
path from s to w? Where there's only 2 
paths at all in the graph. 

138
00:08:01,413 --> 00:08:04,626
The one straight from s to w. 
That has length 2. 

139
00:08:04,626 --> 00:08:09,358
And the one that goes via the 
intermediate point v, that has length 3 + 

140
00:08:09,358 --> 00:08:11,286
-2 =. 
The one which is shorter. 

141
00:08:11,286 --> 00:08:14,057
So svw is the shortest path that has 
length one. 

142
00:08:14,057 --> 00:08:17,824
Y is Dijkstra incorrect. 
Well if you go back to the pseudo code of 

143
00:08:17,824 --> 00:08:22,295
Dijkstra, you'll see that in the very 
first iteration, it will greedily find 

144
00:08:22,295 --> 00:08:25,052
the closest vertex to s, in that case 
this is w. 

145
00:08:25,052 --> 00:08:28,237
W is closer than v. 
It'll greedily compute the shortest path 

146
00:08:28,237 --> 00:08:31,122
distance to w, knowing the information it 
has right now. 

147
00:08:31,122 --> 00:08:33,862
And all it knows is there's this 1 hop 
path from s to w. 

148
00:08:33,862 --> 00:08:37,672
So it'll irrevocably commute to the 
shortest path distance from s to w as 2. 

149
00:08:37,672 --> 00:08:41,547
Never reconsidering that decision later. 
So Dykstra will terminate with the 

150
00:08:41,547 --> 00:08:44,757
incorrect output that the shortest path 
length from s to w is 2. 

151
00:08:44,757 --> 00:08:47,482
This doesn't contradict anything we 
proved in part 1. 

152
00:08:47,482 --> 00:08:49,747
'Cuz we established correctness of 
Dykstra. 

153
00:08:49,747 --> 00:08:53,072
Only under the assumption that all edge 
lengths are not negative. 

154
00:08:53,072 --> 00:08:56,327
An assumption which is. 
Violated in this particular example. 

155
00:08:56,327 --> 00:08:59,927
But again the takeaway point here is 
that, you know, it's easy to write down a 

156
00:08:59,927 --> 00:09:02,827
greedy algorithm, especially if you came 
up with it yourself. 

157
00:09:02,827 --> 00:09:06,552
You probably believe deep in your heart 
that it's got to be correct all the time, 

158
00:09:06,552 --> 00:09:10,177
but more often than not, probably your 
greedy heuristic is nothing more than a 

159
00:09:10,177 --> 00:09:13,427
heuristic and there will be instances on 
which it does the wrong this. 

160
00:09:13,427 --> 00:09:15,863
So keep that in mind. 
In greedy algorithm design. 

161
00:09:15,863 --> 00:09:19,773
So now that my conscious is clear, having 
warned you about the perils of greedy 

162
00:09:19,773 --> 00:09:22,561
algorithm design. 
Let's turn to proofs of correctness. 

163
00:09:22,561 --> 00:09:26,496
That is, if you have a greedy algorithm 
that actually is correct, and we'll see 

164
00:09:26,496 --> 00:09:28,802
some notable examples in the coming 
lectures. 

165
00:09:28,802 --> 00:09:32,633
How would you actually establish that 
fact? Or if you have a greedy algorithm 

166
00:09:32,633 --> 00:09:35,202
and you don't know whether or not it's 
correct. 

167
00:09:35,202 --> 00:09:39,639
How would you approach, try to understand 
which one it is, whether it's correct or 

168
00:09:39,639 --> 00:09:43,234
not? So let me level with you. 
Proving greedy algorithm is correct 

169
00:09:43,234 --> 00:09:47,621
frankly is sort of more art than science. 
So unlike the divide and conquer paradigm 

170
00:09:47,621 --> 00:09:52,269
where everything was somewhat formulaic. 
We had this black box ways of evaluating 

171
00:09:52,269 --> 00:09:55,900
recurrences, we have this So the template 
for proving algorithms correct. 

172
00:09:55,900 --> 00:09:59,441
Really, approving correctness of greedy 
algorithms takes a lot of creativity. 

173
00:09:59,441 --> 00:10:02,894
And it has a bit of an ad hock flavor. 
That's it, as usual, to the extent that 

174
00:10:02,894 --> 00:10:05,667
there are recurring themes. 
That is what I will spend our time 

175
00:10:05,667 --> 00:10:08,344
together emphasising. 
So let me tell you, just a little bit 

176
00:10:08,344 --> 00:10:12,061
again, about very high levels. 
How you might go about this, you again 

177
00:10:12,061 --> 00:10:16,546
might want to revisit this context af, 
content after you've seen some examples 

178
00:10:16,546 --> 00:10:19,061
where I think it will make a lot more 
sense. 

179
00:10:19,061 --> 00:10:23,059
So method one, is our old friend or 
perhaps nemesis depending on your 

180
00:10:23,059 --> 00:10:27,734
disposition, namely proofs by induction. 
Now for greedy algorithms remember that 

181
00:10:27,734 --> 00:10:31,340
they do is they sequentially make a bunch 
of irrevocable decisions. 

182
00:10:31,340 --> 00:10:35,452
So here the induction is going to be on 
the decisions made by the algorithm and 

183
00:10:35,452 --> 00:10:39,199
if you go back to our proof of 
correctness of Dijkstra's algorithm that 

184
00:10:39,199 --> 00:10:42,548
in fact is exactly how we proved 
Dijkstra's algorithm correct. 

185
00:10:42,548 --> 00:10:46,635
It was by induction on the number of 
iterations and each iteration of the main 

186
00:10:46,635 --> 00:10:50,639
while loop we compute the shortest path 
to one new destination and we always 

187
00:10:50,639 --> 00:10:53,942
proved that assuming all of our previous 
computations. 

188
00:10:53,942 --> 00:10:57,606
Were correct, that's the inductive 
hypothesis, then so is the computation 

189
00:10:57,606 --> 00:11:00,952
and the current iteration. 
And so then by induction, everything the 

190
00:11:00,952 --> 00:11:04,769
algorithm ever does is correct. 
So that's a greedy proof by induction 

191
00:11:04,769 --> 00:11:08,561
that a greedy algorithm can be correct, 
and we might see some more examples of 

192
00:11:08,561 --> 00:11:11,652
those and for other algorithms in the 
lectures to come. 

193
00:11:11,652 --> 00:11:15,952
Some of the textbooks call this method of 
proof, greedy stays ahead, meaning you 

194
00:11:15,952 --> 00:11:19,417
always prove greedy's doing the right 
thing iteration by iteration. 

195
00:11:19,417 --> 00:11:23,262
So a second approach to proving the 
correctness of greedy algorithms, which 

196
00:11:23,262 --> 00:11:26,602
works in a lot of cases, is what's called 
an exchange argument. 

197
00:11:26,602 --> 00:11:29,928
Now you haven't yet seen any examples of 
exchange arguments in this class. 

198
00:11:29,928 --> 00:11:33,594
So I can't really tell you what they are. 
But that's what we're going to proceed to 

199
00:11:33,594 --> 00:11:35,355
next. 
I'm going to argue by an exchange 

200
00:11:35,355 --> 00:11:38,729
argument that a couple of different 
famous greedy algorithms are, in fact, 

201
00:11:38,729 --> 00:11:40,851
correct. 
It has a couple of different flavors. 

202
00:11:40,851 --> 00:11:42,948
One flavor is to approach it by 
contradiction. 

203
00:11:42,948 --> 00:11:46,032
You assume for contradiction, that a 
greedy algorithm is incorrect. 

204
00:11:46,032 --> 00:11:48,562
And then you show that you take an 
optimal solution. 

205
00:11:48,562 --> 00:11:52,850
And exchange two elements of that optimal 
solution and get something even better. 

206
00:11:52,850 --> 00:11:56,282
Which of course, contradicts the 
assumption that you started with an 

207
00:11:56,282 --> 00:11:59,656
optimal solution. 
a different flavor would be to gradually 

208
00:11:59,656 --> 00:12:03,624
exchange an optimal solution into the one 
output by a greedy algorithm, without 

209
00:12:03,624 --> 00:12:07,094
making the solution any worse. 
That would show that the up with the 

210
00:12:07,094 --> 00:12:11,388
greedy algorithm is in fact optimal and 
informally that's done by an induction on 

211
00:12:11,388 --> 00:12:15,541
the number of exchanges required to 
transform an optimal solution into yours, 

212
00:12:15,541 --> 00:12:19,844
and finally I've already said it once but 
let me say it again, there is not a whole 

213
00:12:19,844 --> 00:12:23,135
lot of formula behind proving greedy 
algorithm is correct. 

214
00:12:23,135 --> 00:12:27,421
You often have to be quite creative, you 
might have to switch together aspects of 

215
00:12:27,421 --> 00:12:31,367
method 1 and method 2, you might have to 
do something completely different. 

216
00:12:31,367 --> 00:12:33,386
Really, any rigorous fruit is fair gain. 

